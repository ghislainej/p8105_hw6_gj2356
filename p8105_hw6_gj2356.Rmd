---
title: "Homework 6"
author: "Ghislaine Jumonville"
date: "2023-12-01"
output: github_document
---

```{r, include = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
set.seed(123)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 1
The code chunk below loads the Washington Post homicide data and creates a new variable `city_state` that combines the city and state into one variable, and a new binary variable `status` that indicates if the homicide is solved or unsolved. 
```{r, warning = FALSE, message = FALSE}
homicide =
  read_csv("homicide-data.csv") |> 
  janitor::clean_names() |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    status = case_match(disposition,
                       "Open/No arrest" ~ 1,
                       .default = 0),
    victim_age = as.numeric(victim_age),
    victim_race = as.factor(victim_race)) |> 
  filter(!city_state %in% c("Dallas, TX","Phoenix, AZ", "Kansas City, MO","Tulsa, AL"),
         victim_race %in% c("White", "Black")) 
```

Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.

For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of glm as an R object; apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r, warning = FALSE}
baltimore_fit = 
  homicide |> 
  filter(city_state == "Baltimore, MD") |> 
  glm(status ~ victim_age + victim_sex + victim_race, data = _, family = binomial())

broom::tidy(baltimore_fit)
```

estimate = 1.04

```{r}

```

Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

## Problem 2

The code chunk below loads in the weather data and creates a new dataframe `weather_df`.
```{r, warning = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

The code chunk below creates 5000 bootstrap samples with estimates of r-squared and log(β1*β2).
```{r, warning = FALSE}
weather_bootstrap_data = 
weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results_rsquare = map(models, broom::glance),
    results_beta = map(models, broom::tidy)) |> 
  select(results_rsquare, results_beta) |> 
  mutate(
    id = row_number()
  ) |> 
  unnest(results_rsquare) |> 
  select(r.squared, results_beta, id) |> 
  unnest(results_beta) |> 
  select(id, r.squared, term, estimate) |> 
  pivot_wider(names_from = term, values_from = estimate) |> 
  mutate(
    beta_beta = tmin*prcp,
    log_beta_beta = log(beta_beta)
  ) 
```

### Distribution of R-Squared Values

The code chunk below creates a density graph for the r-squared estimates from the 5000 bootstrap samples.
```{r, warning = FALSE}
r_squared_distribution =
  weather_bootstrap_data |> 
  ggplot(aes(x = r.squared)) + 
    geom_density() +
    labs(title = "Distribution of R-Squared")
r_squared_distribution
```

The distribution of r-squared estimates from the 5000 bootstrap samples is approximately normally distributed with a mean around 0.92.

### Distribution of Log(β1*β2) Values

The code chunk below creates a density graph for the log(β1*β2) estimates from the 5000 bootstrap samples.
```{r, warning = FALSE}
log_beta_beta_distribution =
  weather_bootstrap_data |> 
  ggplot(aes(x = log_beta_beta)) + 
    geom_density() + 
    labs(title = "Distribution of log(beta1*beta2)")
log_beta_beta_distribution
```

This distribution is left skewed with most of the values falling between -8 and -4.

The code chunk below creates a 95% confidence interval for the r-squared and log(β1*β2) estimates.
```{r, warning = FALSE}
weather_bootstrap_data |> 
  summarize(
    ci_lower_rsquared = quantile(r.squared, 0.025), 
    ci_upper_rsquared = quantile(r.squared, 0.975),
    ci_lower_logbetabeta = quantile(log_beta_beta, 0.025, na.rm = TRUE),
    ci_upper_logbetabeta = quantile(log_beta_beta, 0.975, na.rm = TRUE)) |> 
  knitr::kable()
```

## Problem 3

The code chunk below loads and cleans the `birthweight.csv` dataset. Sex (`babysex`), father's race(`frace`), and mother's race (`mrace`) were converted to factors.
```{r, warning = FALSE}
birthweight_data =  
  read_csv("birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate(
    babysex = factor(babysex),
    frace = factor(frace),
    mrace = factor(mrace)
  )

skimr::skim(birthweight_data)
```

The code chunk below creates a histogram and boxplot of the birthweights.
```{r, warning = FALSE}
birthweight_data |>
  ggplot(aes(x = bwt)) + 
  geom_histogram(color = "black", fill = "turquoise", alpha = 0.5) +
  labs(
    title = "Distribution of Birthweights",
    x = "Birthweight (grams)",
    y = "Count"
  )

birthweight_data |> 
  ggplot(aes(x = bwt)) + 
  geom_boxplot(color = "black", fill = "turquoise", alpha = 0.5) +
  labs(
    title = "Distribution of Birthweights",
    x = "Birthweight (grams)" 
  )
```
Looking at the histogram of birthweights, it looks approximately normally distributed. Then looking at the boxplot of the birthweights, there are a few outliers on each end of the data. The outliers will likely impact our model and our residuals.


When building this model, I started with only two variables: baby's head circumference at birth (`bhead`) and baby's length at birth (`blength`), which ended up being the two covariates that I settled on for my final model. I considered that the length of the baby as well as its head circumference would be very indicative of the overall birthweight of the baby. I played around with adding other variables, but it did not add much in terms of changing the r-squared value and at times lowered my r-squared value. Therefore, I decided to stick with just the two variables to keep the model as simple as possible while maintaining a high r-squared value.
```{r, warning = FALSE}
birthweight_model_1 = 
  lm(bwt ~ bhead + blength, data = birthweight_data)

broom::glance(birthweight_model_1)
broom::tidy(birthweight_model_1)
```

The code chunk below creates a scatterplot of the residuals agains the predicted values and creates a histogram of the residuals. 
```{r, warning = FALSE}
birthweight_data |> 
  modelr::add_residuals(birthweight_model_1) |> 
  modelr::add_predictions(birthweight_model_1) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(color = "turquoise", alpha = 0.5) +
  labs(
    title = "Scatterplot of Residuals Against Predicted Values",
    x= "Predicted Values",
    y= "Residuals"
  )

birthweight_data |> 
  modelr::add_residuals(birthweight_model_1) |> 
  ggplot(aes(x = resid)) + 
  geom_histogram(color = "black", fill = "turquoise", alpha = 0.5) +
  labs(
    title = "Distribution of Residuals",
    x = "Residuals",
    y = "Count"
  )
```
The scatterplot of the residuals against the predicted values shows most of the residuals randomly clustering around 0, which is a good sign for the model. There is evidence of a few outliers on both ends, but the original dataset contained quite a few outliers and without doing more investigation and analysis there is not much that can be done.

The histogram shows that the residuals are approximately normally distributed, which is further evidence that the model is doing a decent job of fitting the data.

The code chunk below creates a model using length at birth and gestational age as predictors of birthweight.
```{r, warning = FALSE}
birthweight_model_2 =
  lm(bwt ~ blength + gaweeks, data = birthweight_data)

broom::glance(birthweight_model_2)
broom::tidy(birthweight_model_2)
```

The code chunk below creates a model using baby's head circumference, baby's length, the baby's sex, and all the interactions as predictors of birthweight. 
```{r, warning = FALSE}
birthweight_model_3 =
  lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = birthweight_data)

broom::glance(birthweight_model_3)
broom::tidy(birthweight_model_3)
```

The code chunk below generates 100 cross-validation folds for the `birthweight_data` and splits that data into training and testing sets. 
```{r, warning = FALSE}
cv_birthweight_df =
  crossv_mc(birthweight_data, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

The code chunk below performs cross-validated linear regression modeling on the `birthweight_data`, assessing the root mean squared error (RSME) for the three models created. This allows us to assess which models are doing a good job at predicting our data.
```{r, warning = FALSE}
cv_birthweight_df = 
  cv_birthweight_df |> 
  mutate(
    birthweight_model_1  = map(train, \(df) lm(bwt ~ bhead + blength, data = birthweight_data)),
    birthweight_model_2  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = birthweight_data)),
    birthweight_model_3  = map(train, \(df) lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_data))) |> 
  mutate(
    rmse_1 = map2_dbl(birthweight_model_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2 = map2_dbl(birthweight_model_2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_3 = map2_dbl(birthweight_model_3, test, \(mod, df) rmse(model = mod, data = df)))
```

The code chunk below creates a violin plots for each of the models. 
```{r, warning = FALSE}
cv_birthweight_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
    geom_violin(fill = "turquoise", alpha = 0.5) +
    labs(
      x = "Model",
      y = "Root Mean Squared Error (RMSE)"
    )
```

Looking at this plot, Model 1 and Model 3 have roughly similar prediction accuracies, while Model 2 has the worse prediction accuracy. Model 3 includes 7 predictors (3 main effects) and only appears to only be slightly better at predicting birthweight than Model 1 which contains 2 predictors (main effects only). 

